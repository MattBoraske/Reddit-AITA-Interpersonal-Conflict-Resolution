{"cells":[{"cell_type":"markdown","id":"iNi9K61Umwlk","metadata":{"id":"iNi9K61Umwlk"},"source":["# Perplexity analysis of Llama-2-13B-Chat when finetuned on separate AITA class partitions of Reddit AITA dataset"]},{"cell_type":"code","execution_count":null,"id":"a38d4cf8-bf07-492e-a50f-d4d63927406e","metadata":{"id":"a38d4cf8-bf07-492e-a50f-d4d63927406e","outputId":"8191eb54-9b1a-43c2-c15e-2fe8654c5371"},"outputs":[],"source":["!pip install transformers datasets tqdm accelerate bitsandbytes torch evaluate"]},{"cell_type":"code","execution_count":null,"id":"c32697f9-6df1-4fe0-b2b6-c05b2b63bc6b","metadata":{"colab":{"referenced_widgets":["af4dc2c3d9b6434e8a78fd18c38e143e"]},"id":"c32697f9-6df1-4fe0-b2b6-c05b2b63bc6b","outputId":"9e600ed3-5947-401e-ce86-9789285fcaab"},"outputs":[],"source":["from huggingface_hub import login\n","\n","login()"]},{"cell_type":"code","execution_count":null,"id":"26f12ebe-1df8-4073-a118-59e68a8153d7","metadata":{"id":"26f12ebe-1df8-4073-a118-59e68a8153d7"},"outputs":[],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"MattBoraske/Reddit-AITA-2018-to-2022\")\n","test_dataset = dataset['test']\n","\n","test_datasets = {\n","    'NTA': test_dataset.filter(lambda x: x['top_comment_1_AITA_class_by_keyword'] == 'NTA'),\n","    'YTA': test_dataset.filter(lambda x: x['top_comment_1_AITA_class_by_keyword'] == 'YTA'),\n","    'ESH': test_dataset.filter(lambda x: x['top_comment_1_AITA_class_by_keyword'] == 'ESH'),\n","    'NAH': test_dataset.filter(lambda x: x['top_comment_1_AITA_class_by_keyword'] == 'NAH'),\n","}\n","\n","\n","def create_input_text(example):\n","    example['input_text'] = example['submission_title'] + \" \" + example['submission_text']\n","    return example\n","\n","for key in test_datasets:\n","    test_datasets[key] = test_datasets[key].map(create_input_text)\n","\n","def get_llama2_training_instruction(sample):\n","    llama2_instruction = f\"<s>[INST] {sample['input_text']} [/INST] {sample['top_comment_1']} </s>\"\n","    return {\"llama2_instruction\": llama2_instruction}\n","\n","for key in test_datasets:\n","    test_datasets[key] = test_datasets[key].map(get_llama2_training_instruction)\n","\n","def drop_columns(dataset, columns):\n","    return dataset.remove_columns(columns)\n","\n","columns_to_drop = ['input_text', 'submission_title', 'submission_text', 'submission_score', 'submission_url', 'submission_date', 'top_comment_1', 'top_comment_2', 'top_comment_3', 'top_comment_4', 'top_comment_5', 'top_comment_6', 'top_comment_7', 'top_comment_8', 'top_comment_9', 'top_comment_10', 'top_comment_1_AITA_class_by_keyword', 'top_comment_2_AITA_class_by_keyword', 'top_comment_3_AITA_class_by_keyword', 'top_comment_4_AITA_class_by_keyword', 'top_comment_5_AITA_class_by_keyword', 'top_comment_6_AITA_class_by_keyword', 'top_comment_7_AITA_class_by_keyword', 'top_comment_8_AITA_class_by_keyword', 'top_comment_9_AITA_class_by_keyword', 'top_comment_10_AITA_class_by_keyword']\n","\n","for key in test_datasets:\n","    test_datasets[key] = drop_columns(test_datasets[key], columns_to_drop)\n","\n","test_datasets = {key: dataset['llama2_instruction'] for key, dataset in test_datasets.items()}"]},{"cell_type":"code","execution_count":null,"id":"e2ae80b2-b460-48a2-9442-5918b1fb81ef","metadata":{"colab":{"referenced_widgets":["1ac74312595d4abeb4dc3493f957e8a0","fa55c45be09e4b80b2758e39235124ce","404a0c9d7eaa43719aa0c89f1d4134a5","215fcbe91d674b808c3d8e8e376e7585","085a4cf71210495c8ca8685f6acc6f27","f5a46859781543b9b06451173fb1623d","d693a21dfad84101b9396b5e961a4725","27c0d93084424f15afca4d4de71f9e9c","68bbc9928677496192e8b8697a19874a","6d60cc119c2b49e29455b50121c027e8","c9d650362c0f4b9a8b00a91ba2d1668e","a238199af17143dca6acf83c35da7ac7","f6a2b47c08d140a39eb952a51f63d37f","5d79a7646561412eb0660bd5dff2c2dc","1e7980f2ad664ee9beff02887be52a4b"]},"id":"e2ae80b2-b460-48a2-9442-5918b1fb81ef","outputId":"60c1105a-b17d-4282-fd3c-485f0cb26699"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"MattBoraske/llama-2-13b-chat-reddit-AITA-NAH\")\n","model = AutoModelForCausalLM.from_pretrained(\"MattBoraske/llama-2-13b-chat-reddit-AITA-NAH\").cuda()"]},{"cell_type":"code","execution_count":null,"id":"1bbb010a-895e-40c0-bf82-39475cddafd2","metadata":{"id":"1bbb010a-895e-40c0-bf82-39475cddafd2","outputId":"1eb23664-2537-42e6-f46c-c85462ff5eed"},"outputs":[],"source":["import torch\n","from tqdm import tqdm\n","\n","avg_perplexities = []\n","\n","for AITA_class, test_data in test_datasets.items():\n","    nlls = []\n","\n","    for sample in tqdm(test_data):\n","        # Ensure the input_ids are on the GPU\n","        input_ids = tokenizer(sample, max_length=4096, return_tensors=\"pt\").input_ids.cuda()\n","        target_ids = input_ids.clone().cuda()  # Cloning and ensuring it's on GPU\n","\n","        with torch.no_grad():\n","            outputs = model(input_ids, labels=target_ids)\n","\n","            # Loss should already be on the correct device, but we ensure it\n","            neg_log_likelihood = outputs.loss.to(input_ids.device)\n","\n","        nlls.append(neg_log_likelihood)\n","\n","    nlls_tensor = torch.stack(nlls).to(input_ids.device)\n","    ppl = torch.exp(nlls_tensor.mean())\n","    print(f\"Avg Perplexity for {AITA_class} samples: {ppl.item()}\")\n","    avg_perplexities.append({AITA_class, ppl})\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}
