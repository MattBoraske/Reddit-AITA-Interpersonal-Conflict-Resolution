{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit AITA Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets transformers pandas numpy huggingface_hub ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading of inital dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# load inital datafile\n",
    "initial_datafile = 'aita-datafiles/Reddit_AITA_2018_to_2022.csv' \n",
    "dataset = Dataset.from_pandas(pd.read_csv(initial_datafile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter #1: Removal of samples where any of the submission titles, text, or top 10 comments don't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove samples where any of the top comments are None or empty\n",
    "\n",
    "def remove_none_comments(example):\n",
    "    keys_to_check = [\n",
    "        'submission_title', 'submission_text', 'top_comment_1', 'top_comment_2', 'top_comment_3',\n",
    "        'top_comment_4', 'top_comment_5', 'top_comment_6', 'top_comment_7', 'top_comment_8',\n",
    "        'top_comment_9', 'top_comment_10'\n",
    "    ]\n",
    "    return all(example.get(key) not in (None, '') for key in keys_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(remove_none_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter #2: Removal of samples where any comments did not contain an AITA decision\n",
    "- identified using earliest keyword matching according to the phrases for each AITA class in \"classes_dictionary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Creation of AITA Decision Columns For Each Comment Using Earliest Keyword Matching\n",
    "\n",
    "classes_dictionary = {\n",
    "    'NTA': ['not the asshole', 'not the a**hole', 'not the a-hole', 'you would not be the asshole', 'you would not be the a**hole', 'you would not be the a-hole', 'not an asshole', 'not an a**hole', 'not an a-hole', 'you would not be an asshole', 'you would not be an a**hole', 'you would not be an a-hole', 'nta', 'n t a', 'ywnbta', 'y w n b t a'],\n",
    "    'NAH': ['no assholes here', 'no a**holes here', 'no a-holes here', 'no one is the asshole', 'no one is the a**hole', 'no one is the a-hole', 'no one would be the asshole', 'no one would be the a**hole', 'no one would be the a-hole', 'no one is an asshole', 'no one is an a**hole', 'no one is an a-hole', 'no one would be an asshole', 'no one would be an a**hole', 'no one would be an a-hole', 'nah', 'n a h'],\n",
    "    'ESH': ['everyone sucks here', 'everyone is the asshole', 'everyone is the a**hole', 'everyone is the a-hole', 'everyone would be the asshole', 'everyone would be the a**hole', 'everyone would be the a-hole', 'everyone is an asshole', 'everyone is an a**hole', 'everyone is an a-hole', 'everyone would be an asshole', 'everyone would be an a**hole', 'everyone would be an a-hole', 'esh', 'e s h'],\n",
    "    'YTA': ['you\\'re the asshole', 'you\\'re the a**hole', 'you\\'re the a-hole', 'youre the asshole', 'youre the a**hole', 'youre the a-hole', 'you are the asshole', 'you are the a**hole', 'you are the a-hole', 'you would be the asshole', 'you would be the a**hole', 'you would be the a-hole', 'you the asshole', 'you the a**hole', 'you the a-hole', 'you\\'re an asshole', 'you\\'re an a**hole', 'you\\'re an a-hole', 'youre an asshole', 'youre an a**hole', 'youre an a-hole', 'you are an asshole', 'you are an a**hole', 'you are an a-hole', 'you would be an asshole', 'you would be an a**hole', 'you would be an a-hole', 'you an asshole', 'you an a**hole', 'you an a-hole', 'yta', 'y t a', 'ywbta', 'y w b t a']\n",
    "}\n",
    "\n",
    "def find_earliest_classification(text):\n",
    "    '''\n",
    "    Find the earliest AITA classification in a text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to search for AITA classifications in.\n",
    "\n",
    "    Returns:\n",
    "        str: The earliest classification found in the text.\n",
    "    '''\n",
    "\n",
    "    # track earliest match\n",
    "    earliest_match = None\n",
    "    earliest_match_pos = float('inf')  # Initially set to infinity\n",
    "\n",
    "    # convert input text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # go through all classifications and their keywords\n",
    "    for key, phrases in classes_dictionary.items():\n",
    "        # Create a regex pattern that includes the classification keywords\n",
    "        pattern = r'\\b(' + '|'.join(map(re.escape, phrases)) + r')\\b'\n",
    "\n",
    "        # Search for any keywords in the input text\n",
    "        for match in re.finditer(pattern, text, re.IGNORECASE):\n",
    "            if match.start() < earliest_match_pos:\n",
    "                # Update the earliest match if this match is earlier\n",
    "                earliest_match = key\n",
    "                earliest_match_pos = match.start()\n",
    "\n",
    "    # return the class that had the earliest match\n",
    "    return earliest_match\n",
    "\n",
    "def add_classification(row):\n",
    "    '''\n",
    "    Add comment AITA classifications to a row in the dataset.\n",
    "\n",
    "    Args:\n",
    "        row (dict): A row from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        dict: The row with comment AITA classifications added.\n",
    "    '''\n",
    "    # Iterate over top 10 comment keys\n",
    "    for i in range(1, 11):\n",
    "        key = f'top_comment_{i}'\n",
    "        classification_key = key + '_AITA_class_by_keyword'\n",
    "        if key in row and isinstance(row[key], str): # should be true since we guaranteed that all comments are strings earlier\n",
    "            # if this row has a top_comment_N key, get the classification and add it to the row\n",
    "            classification = find_earliest_classification(row[key])\n",
    "            row[classification_key] = classification\n",
    "        else:\n",
    "            # If the top_comment_N key doesn't exist or isn't a string, set the classification key to None\n",
    "            row[classification_key] = None\n",
    "\n",
    "    # return the row with the classification added\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(add_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_comments_with_no_AITA_class_keyword(example):\n",
    "    if example['top_comment_1_AITA_class_by_keyword'] is None or example['top_comment_2_AITA_class_by_keyword'] is None or example['top_comment_3_AITA_class_by_keyword'] is None or example['top_comment_4_AITA_class_by_keyword'] is None or example['top_comment_5_AITA_class_by_keyword'] is None or example['top_comment_6_AITA_class_by_keyword'] is None or example['top_comment_7_AITA_class_by_keyword'] is None or example['top_comment_8_AITA_class_by_keyword'] is None or example['top_comment_9_AITA_class_by_keyword'] is None or example['top_comment_10_AITA_class_by_keyword'] is None:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(remove_comments_with_no_AITA_class_keyword)\n",
    "dataset = dataset.remove_columns('__index_level_0__')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter #3: Removal of samples where the submission title or text have been removed or deleted\n",
    "- Comments are not of concern since any samples that have comment(s) which have been deleted or removed have already been filtered out due to having comment(s) that don't contain an AITA class keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the most popular submission titles and texts to determine what Reddit uses for posts that are deleted or removed\n",
    "## indicates that...\n",
    "###  for submission titles this is done by either [deleted by user] or [ Removed by Reddit ]\n",
    "###  for submission text this is done by either [deleted], [removed], or '.' (a single period)\n",
    "\n",
    "df = dataset.to_pandas()\n",
    "print(f'MOST POPULAR SUBMISSION TITLES\\n {df[\"submission_title\"].value_counts()[:20]}')\n",
    "print()\n",
    "print(f'MOST POPULAR SUBMISSION TEXTS\\n {df[\"submission_text\"].value_counts()[:20]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove samples where the submission text is [deleted], [removed], or '.' (a single period) or the submission title is [deleted by user] or [ Removed by Reddit ]\n",
    "\n",
    "def remove_deleted_removed_posts(example):\n",
    "    if example['submission_text'] in ['[deleted]', '[removed]', '.'] or example['submission_title'] in ['[deleted by user]', '[ Removed by Reddit ]']:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "dataset = dataset.filter(remove_deleted_removed_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter #3: Removal of Edits in both Submission Texts and Top Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_edits(text):\n",
    "  \"\"\"\n",
    "  Removes the edits portion of a text\n",
    "\n",
    "  Parameters:\n",
    "    text: A string containing the text.\n",
    "\n",
    "  Returns:\n",
    "    A string with the edits removed, if present.\n",
    "  \"\"\"\n",
    "\n",
    "  global edits_removed_counter\n",
    "\n",
    "  if text == None:\n",
    "    return text\n",
    "\n",
    "  text = text.lower()\n",
    "\n",
    "  pattern = r\"(edit:|edit -|edit-|eta:|eta -|eta-|edited:|edited -|edited-|edit after:|edit after- |edit after -|edit afterwards:|edit afterwards -|edit afterwards-|edited to add:|edited to add -|edited to add-|update:|update-|update -|updated:|updated-|updated -)\"\n",
    "  match = re.search(pattern, text, flags=re.IGNORECASE)\n",
    "  if match:\n",
    "      edits_removed_counter += 1 # increment the edits_removed_counter\n",
    "      return text[:match.start()].strip() # return the text up to the start of the match\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits_removed_counter = 0\n",
    "\n",
    "dataset = dataset.map(lambda x: {\"submission_title\": remove_edits(x[\"submission_title\"])})\n",
    "dataset = dataset.map(lambda x: {\"submission_text\": remove_edits(x[\"submission_text\"])})\n",
    "for i in range(1, 11):\n",
    "    dataset = dataset.map(lambda x: {f\"top_comment_{i}\": remove_edits(x[f\"top_comment_{i}\"])})\n",
    "\n",
    "print(f\"Number of edits removed: {edits_removed_counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove samples where the removal of edits resulted in empty submission titles, texts, or top comments\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "columns = [f'top_comment_{i}' for i in range(1, 11)]\n",
    "columns.append('submission_title')\n",
    "columns.append('submission_text')\n",
    "empty_string_indices = df[columns].apply(lambda row: any(cell == '' for cell in row), axis=1).index[df[columns].apply(lambda row: any(cell == '' for cell in row), axis=1)].tolist()\n",
    "\n",
    "df = df[~df.index.isin(empty_string_indices)]\n",
    "\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter #4: Removal of outliers (upper and lower 2.5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove outlier samples based on submission text length\n",
    "import numpy as np\n",
    "\n",
    "submission_text_lengths = np.array([len(text) for text in dataset['submission_text']])\n",
    "UPPER_BOUND = 97.5\n",
    "LOWER_BOUND = 2.5\n",
    "dataset = dataset.filter(lambda x: len(x['submission_text']) >= np.percentile(submission_text_lengths, LOWER_BOUND) and len(x['submission_text']) <= np.percentile(submission_text_lengths, UPPER_BOUND))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "- 10% of each AITA class (NTA, YTA, ESH, NAH) in test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "# Split the dataset based on the values of 'top_comment_1_AITA_class_by_keyword'\n",
    "nta_dataset = dataset['train'].filter(lambda x: x['top_comment_1_AITA_class_by_keyword'] == 'NTA')\n",
    "yta_dataset = dataset['train'].filter(lambda x: x['top_comment_1_AITA_class_by_keyword'] == 'YTA')\n",
    "esh_dataset = dataset['train'].filter(lambda x: x['top_comment_1_AITA_class_by_keyword'] == 'ESH')\n",
    "nah_dataset = dataset['train'].filter(lambda x: x['top_comment_1_AITA_class_by_keyword'] == 'NAH')\n",
    "\n",
    "# Create a new DatasetDict with the split datasets\n",
    "split_dataset = DatasetDict({\n",
    "    'train': DatasetDict({\n",
    "        'NTA': nta_dataset,\n",
    "        'YTA': yta_dataset,\n",
    "        'ESH': esh_dataset,\n",
    "        'NAH': nah_dataset\n",
    "    }),\n",
    "})\n",
    "\n",
    "dataset = split_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload dataset to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.push_to_hub('MattBoraske/Reddit-AITA-2018-to-2022')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
